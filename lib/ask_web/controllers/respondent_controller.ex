defmodule AskWeb.RespondentController do
  use AskWeb, :api_controller
  require Ask.RespondentStats

  alias Ask.{
    ActivityLog,
    CompletedRespondents,
    Logger,
    Questionnaire,
    Respondent,
    RespondentDispositionHistory,
    Stats,
    Survey,
    SurveyLogEntry,
    RespondentsFilter
  }

  def index(conn, %{"project_id" => project_id, "survey_id" => survey_id} = params) do
    limit = Map.get(params, "limit", "")
    page = Map.get(params, "page", "")
    sort_by = Map.get(params, "sort_by", "")
    sort_asc = Map.get(params, "sort_asc", "")
    q = Map.get(params, "q", "")

    filter = RespondentsFilter.parse(q)
    filter_where = RespondentsFilter.filter_where(filter)

    filtered_query =
      conn
      |> load_project(project_id)
      |> load_survey(survey_id)
      |> assoc(:respondents)
      |> preload(:responses)
      |> preload(:questionnaire)
      |> where(^filter_where)

    respondents_count = Repo.aggregate(filtered_query, :count, :id)

    respondents =
      filtered_query
      |> conditional_limit(limit)
      |> conditional_page(limit, page)
      |> sort_respondents(sort_by, sort_asc)
      |> Repo.all()
      |> Enum.map(fn respondent ->
        respondent
        |> mask_phone_numbers
        |> effective_stats
      end)

    survey = Repo.get!(Survey, survey_id) |> Repo.preload(:questionnaires)

    partial_relevant_enabled = Survey.partial_relevant_enabled?(survey, true)

    render(conn, "index.json",
      respondents: respondents,
      respondents_count: respondents_count,
      partial_relevant_enabled: partial_relevant_enabled,
      index_fields:
        index_fields_for_render(%{
          survey: survey,
          partial_relevant_enabled: partial_relevant_enabled
        })
    )
  end

  defp index_fields_for_render(%{
         survey: survey,
         partial_relevant_enabled: partial_relevant_enabled
       }),
       do:
         index_fields_for_render("fixed") ++
           index_fields_for_render("mode", survey.mode) ++
           index_fields_for_render("variant", survey.comparisons) ++
           index_fields_for_render("partial_relevant", partial_relevant_enabled) ++
           index_fields_for_render("response", survey.questionnaires)

  defp index_fields_for_render("fixed" = field_type),
    do:
      ["phone_number", "disposition", "date"]
      |> map_fields_with_type(field_type)

  defp index_fields_for_render("mode" = field_type, survey_modes),
    do:
      List.flatten(survey_modes)
      |> Enum.uniq()
      |> map_fields_with_type(field_type)

  defp index_fields_for_render("response" = field_type, questionnaires) do
    order_alphabetically = &(String.downcase(&1) < String.downcase(&2))

    all_questionnaires_fields(questionnaires)
    |> Enum.sort(&order_alphabetically.(&1, &2))
    |> map_fields_with_type(field_type)
  end

  defp index_fields_for_render("variant" = _field_type, [] = _survey_comparisons), do: []

  defp index_fields_for_render("variant" = field_type, _survey_comparisons),
    do: [index_field_for_render(field_type, "variant")]

  defp index_fields_for_render("partial_relevant" = field_type, true),
    do: map_fields_with_type(["answered_questions"], field_type)

  defp index_fields_for_render("partial_relevant" = _field_type, _), do: []

  defp map_fields_with_type(field_keys, field_type),
    do: Enum.map(field_keys, fn field_key -> index_field_for_render(field_type, field_key) end)

  defp index_field_for_render(field_type, field_key), do: %{type: field_type, key: field_key}

  defp effective_stats(respondent) do
    effective_stats =
      case respondent.stats do
        %{attempts: %{"ivr" => _ivr_attempts}} = stats ->
          effective_attempts = Map.put(stats.attempts, "ivr", Ask.Stats.attempts(stats, :ivr))
          %{stats | attempts: effective_attempts}

        stats ->
          stats
      end

    %{respondent | stats: effective_stats}
  end

  defp sort_respondents(query, sort_by, sort_asc) do
    case {sort_by, sort_asc} do
      {"date", "true"} ->
        query |> order_by([r], asc: r.updated_at)

      {"date", "false"} ->
        query |> order_by([r], desc: r.updated_at)

      {"disposition", "true"} ->
        query |> order_by([r], asc: r.disposition)

      {"disposition", "false"} ->
        query |> order_by([r], desc: r.disposition)

      _ ->
        query
    end
  end

  def stats(conn, %{"project_id" => project_id, "survey_id" => survey_id}) do
    survey =
      conn
      |> load_project(project_id)
      |> load_survey(survey_id)

    stats(conn, survey, survey.quota_vars)
  rescue
    e ->
      Logger.error(
        e,
        __STACKTRACE__,
        "Error occurred while processing respondent stats (survey_id: #{survey_id})"
      )

      Sentry.capture_exception(e,
        stacktrace: System.stacktrace(),
        extra: %{survey_id: survey_id}
      )

      render(conn, "stats.json", stats: nil)
  end

  defp stats(conn, survey, []) do
    questionnaires = (survey |> Repo.preload(:questionnaires)).questionnaires
    respondent_count = Ask.RespondentStats.respondent_count(survey_id: ^survey.id)

    cond do
      length(questionnaires) > 1 && length(survey.mode) > 1 ->
        stats(
          conn,
          survey,
          respondent_count,
          respondents_by_questionnaire_mode_and_disposition(survey),
          respondents_by_questionnaire_mode_and_completed_at(survey),
          reference(questionnaires, survey.mode),
          [],
          "stats.json"
        )

      length(survey.mode) > 1 ->
        stats(
          conn,
          survey,
          respondent_count,
          respondents_by_mode_and_disposition(survey),
          respondents_by_mode_and_completed_at(survey),
          reference(questionnaires, survey.mode),
          [],
          "stats.json"
        )

      true ->
        stats(
          conn,
          survey,
          respondent_count,
          respondents_by_questionnaire_and_disposition(survey),
          respondents_by_questionnaire_and_completed_at(survey),
          reference(questionnaires, survey.mode),
          [],
          "stats.json"
        )
    end
  end

  defp stats(conn, survey, _) do
    buckets = (survey |> Repo.preload(:quota_buckets)).quota_buckets

    empty_bucket_ids =
      buckets |> Enum.filter(fn bucket -> bucket.quota in [0, nil] end) |> Enum.map(& &1.id)

    buckets = buckets |> Enum.reject(fn bucket -> bucket.quota in [0, nil] end)
    respondent_count = Ask.RespondentStats.respondent_count(survey_id: ^survey.id)

    stats(
      conn,
      survey,
      respondent_count,
      respondents_by_bucket_and_disposition(survey, empty_bucket_ids),
      respondents_by_quota_bucket_and_completed_at(survey, empty_bucket_ids),
      buckets,
      buckets,
      "quotas_stats.json"
    )
  end

  defp stats(
         conn,
         survey,
         total_respondents,
         respondents_by_disposition,
         respondents_by_completed_at,
         references,
         buckets,
         layout
       ) do
    target = target(survey, buckets, total_respondents)

    total_respondents_by_disposition = Ask.RespondentStats.respondents_by_disposition(survey)

    # Completion percentage
    attempted_respondents =
      total_respondents_by_disposition
      |> Enum.filter(fn {d, _} -> d not in [:queued, :registered] end)
      |> Enum.map(fn {_, c} -> c end)
      |> Enum.sum()

    dispositions_for_completion = Respondent.completed_dispositions(survey.count_partial_results)

    completed_or_partial =
      total_respondents_by_disposition
      |> Enum.filter(fn {d, _} -> d in dispositions_for_completion end)
      |> Enum.map(fn {_, c} -> c end)
      |> Enum.sum()

    grouped_respondents =
      respondents_by_completed_at
      |> Enum.reduce(%{}, fn {group_id, date, count}, acc ->
        {_, next_acc} =
          acc
          |> Map.get_and_update(group_id, fn counts_by_date ->
            {counts_by_date, (counts_by_date || []) ++ [{date, count}]}
          end)

        next_acc
      end)

    stats = %{
      id: survey.id,
      reference: references,
      respondents_by_disposition:
        respondent_counts(respondents_by_disposition, total_respondents),
      cumulative_percentages:
        cumulative_percentages(references, grouped_respondents, survey, target, buckets),
      completion_percentage: completed_or_partial / target * 100,
      total_respondents: total_respondents,
      target: target,
      attempted_respondents: attempted_respondents
    }

    render(conn, layout, stats: stats)
  end

  defp reference([questionnaire], [modes]) do
    [
      %{
        id: questionnaire.id,
        name: questionnaire.name,
        modes: modes
      }
    ]
  end

  defp reference([_], mode) do
    mode
    |> Enum.map(fn modes ->
      %{
        id: modes |> Enum.join(""),
        modes: modes
      }
    end)
  end

  defp reference(questionnaires, [_]) do
    questionnaires
    |> Enum.map(fn q ->
      %{id: q.id, name: q.name}
    end)
  end

  defp reference(questionnaires, mode) do
    questionnaires
    |> Enum.reduce([], fn questionnaire, reference ->
      mode
      |> Enum.reduce(reference, fn modes, reference ->
        reference ++
          [
            %{
              id: "#{questionnaire.id}#{modes |> Enum.join("")}",
              name: questionnaire.name,
              modes: modes
            }
          ]
      end)
    end)
  end

  defp target(survey, buckets, total_respondents) do
    total_quota =
      buckets
      |> Enum.reduce(0, fn bucket, total ->
        total + (bucket.quota || 0)
      end)

    cond do
      total_quota > 0 -> total_quota
      !is_nil(survey.cutoff) -> survey.cutoff
      true -> total_respondents
    end
  end

  defp respondent_counts(grouped_responents, total_respondents) do
    respondent_counts =
      grouped_responents
      |> Enum.reduce(%{}, fn {state, disposition, reference_id, count}, counts ->
        disposition = disposition || state

        default_for_disposition = %{disposition => %{by_reference: %{}, count: 0}}
        counts = default_for_disposition |> Map.merge(counts)

        default_for_quota_bucket = %{to_string(reference_id) => count}

        by_reference =
          default_for_quota_bucket
          |> Map.merge(counts[disposition][:by_reference], fn _, count, current_count ->
            count + current_count
          end)

        disposition_total_count = counts[disposition][:count] + count

        counts
        |> Map.put(disposition, %{by_reference: by_reference, count: disposition_total_count})
      end)

    dispositions = [
      "registered",
      "queued",
      "failed",
      "contacted",
      "unresponsive",
      "started",
      "ineligible",
      "rejected",
      "breakoff",
      "refused",
      "interim partial",
      "partial",
      "completed"
    ]

    respondent_counts =
      dispositions
      |> Enum.reduce(respondent_counts, fn disposition, respondent_counts ->
        respondent_counts
        |> Map.put_new(disposition, %{
          by_reference: %{},
          count: 0
        })
      end)

    respondent_counts = add_disposition_percent(respondent_counts, total_respondents)

    groups = %{
      uncontacted: ["registered", "queued", "failed"],
      contacted: ["contacted", "unresponsive"],
      responsive: [
        "started",
        "ineligible",
        "rejected",
        "breakoff",
        "refused",
        "interim partial",
        "partial",
        "completed"
      ]
    }

    groups
    |> Enum.map(fn {name, dispositions} ->
      {count, percent, detail} =
        dispositions
        |> Enum.reduce({0, 0, %{}}, fn disposition, {count_sum, percent_sum, detail} ->
          {respondent_counts[disposition][:count] + count_sum,
           respondent_counts[disposition][:percent] + percent_sum,
           Map.put(detail, disposition, respondent_counts[disposition])}
        end)

      {name, %{count: count, percent: percent, detail: detail}}
    end)
    |> Enum.into(%{})
  end

  defp percent_provider(_group_id, [], target, []) do
    fn count ->
      count / target * 100
    end
  end

  defp percent_provider(group_id, [], _target, buckets) do
    bucket = buckets |> Enum.find(fn bucket -> bucket.id == group_id end)

    fn count ->
      count / bucket.quota * 100
    end
  end

  defp percent_provider(group_id, comparisons, target, []) do
    comparison =
      comparisons
      |> Enum.find(fn comparison ->
        group_id == "#{comparison["questionnaire_id"]}#{comparison["mode"] |> Enum.join("")}" ||
          group_id == comparison["questionnaire_id"] ||
          group_id == comparison["mode"] |> Enum.join("")
      end)

    fn count ->
      count / (comparison["ratio"] * target / 100) * 100
    end
  end

  defp cumulative_percentages(_, _, %{started_at: nil}, _, _), do: %{}

  defp cumulative_percentages(
         references,
         grouped_respondents,
         %{started_at: started_at, comparisons: comparisons, state: state},
         target,
         buckets
       ) do
    grouped_respondents
    |> Enum.into(Enum.map(references, fn reference -> {reference.id, []} end) |> Enum.into(%{}))
    |> Enum.map(fn {group_id, percents_by_date} ->
      # To make sure the series starts at the same time that the survey
      percents_by_date = [{started_at |> DateTime.to_date(), 0}] ++ percents_by_date

      percents_by_date =
        if state == "running" do
          percents_by_date ++ [{DateTime.utc_now() |> DateTime.to_date(), 0}]
        else
          percents_by_date
        end

      {group_id,
       cumulative_percents_for_group(
         percents_by_date,
         percent_provider(group_id, comparisons, target, buckets)
       )}
    end)
    |> Enum.into(%{})
  end

  defp cumulative_percents_for_group(
         percents_by_date,
         percent_provider,
         cumulative_percent \\ 0,
         result \\ []
       )

  defp cumulative_percents_for_group(_percents_by_date, _percent_provider, 100.0, result) do
    result
  end

  defp cumulative_percents_for_group(
         [{date, count}],
         percent_provider,
         cumulative_percent,
         result
       ) do
    result ++ [{date, cumulative_percent |> add_percent(count, percent_provider)}]
  end

  defp cumulative_percents_for_group(
         [{first_date, first_count}, {second_date, second_count} | rest_percents_by_date],
         percent_provider,
         cumulative_percent,
         result
       ) do
    case Date.diff(second_date, first_date) do
      0 ->
        # We already had results for that date and we duplicated the dot
        cumulative_percents_for_group(
          [{first_date, max(first_count, second_count)} | rest_percents_by_date],
          percent_provider,
          cumulative_percent,
          result
        )

      1 ->
        cumulative_percent = cumulative_percent |> add_percent(first_count, percent_provider)

        cumulative_percents_for_group(
          [{second_date, second_count} | rest_percents_by_date],
          percent_provider,
          cumulative_percent,
          result ++ [{first_date, cumulative_percent}]
        )

      _ ->
        cumulative_percent = cumulative_percent |> add_percent(first_count, percent_provider)

        if second_count > 0 do
          cumulative_percents_for_group(
            [
              {second_date |> Timex.shift(days: -1), 0},
              {second_date, second_count} | rest_percents_by_date
            ],
            percent_provider,
            cumulative_percent,
            result ++ [{first_date, cumulative_percent}]
          )
        else
          # We injected today's date with no aditional results
          # there's no need for an additional point to mark the horizontal line
          cumulative_percents_for_group(
            [
              {second_date, second_count} | rest_percents_by_date
            ],
            percent_provider,
            cumulative_percent,
            result ++ [{first_date, cumulative_percent}]
          )
        end
    end
  end

  defp add_percent(cumulative_percent, count, percent_provider) do
    (cumulative_percent + percent_provider.(count))
    |> min(100.0)
  end

  defp respondents_by_questionnaire_and_disposition(survey) do
    Ask.RespondentStats.respondent_count(
      survey_id: ^survey.id,
      by: [:state, :disposition, :questionnaire_id]
    )
  end

  defp respondents_by_questionnaire_mode_and_disposition(survey) do
    Ask.RespondentStats.respondent_count(
      survey_id: ^survey.id,
      by: [:state, :disposition, :questionnaire_id, :mode]
    )
    |> Enum.map(fn {state, disposition, questionnaire_id, mode, count} ->
      reference_id =
        if mode && questionnaire_id do
          "#{questionnaire_id}#{mode |> Poison.decode!() |> Enum.join("")}"
        else
          nil
        end

      {state, disposition, reference_id, count}
    end)
  end

  defp respondents_by_mode_and_disposition(survey) do
    Ask.RespondentStats.respondent_count(survey_id: ^survey.id, by: [:state, :disposition, :mode])
  end

  defp respondents_by_bucket_and_disposition(survey, bucket_ids) do
    Ask.RespondentStats.respondent_count(
      survey_id: ^survey.id,
      quota_bucket_id: not_in_list(^bucket_ids),
      by: [:state, :disposition, :quota_bucket_id]
    )
  end

  defp respondents_by_questionnaire_and_completed_at(survey) do
    Repo.all(
      from r in CompletedRespondents,
        where: r.survey_id == ^survey.id,
        group_by: [r.questionnaire_id, r.date],
        order_by: r.date,
        select: {r.questionnaire_id, r.date, fragment("CAST(? AS UNSIGNED)", sum(r.count))}
    )
  end

  defp respondents_by_questionnaire_mode_and_completed_at(survey) do
    Repo.all(
      from r in CompletedRespondents,
        where: r.survey_id == ^survey.id and r.questionnaire_id != 0 and r.mode != "",
        group_by: [r.questionnaire_id, r.mode, r.date],
        order_by: r.date,
        select:
          {r.questionnaire_id, r.mode, r.date, fragment("CAST(? AS UNSIGNED)", sum(r.count))}
    )
    |> Enum.map(fn {questionnaire_id, mode, completed_at, count} ->
      {"#{questionnaire_id}#{mode |> Poison.decode!() |> Enum.join("")}", completed_at, count}
    end)
  end

  defp respondents_by_mode_and_completed_at(survey) do
    Repo.all(
      from r in CompletedRespondents,
        where: r.survey_id == ^survey.id,
        group_by: [r.mode, r.date],
        order_by: r.date,
        select: {r.mode, r.date, fragment("CAST(? AS UNSIGNED)", sum(r.count))}
    )
    |> Enum.map(fn {mode, completed_at, count} ->
      {mode |> Poison.decode!() |> Enum.join(""), completed_at, count}
    end)
  end

  defp respondents_by_quota_bucket_and_completed_at(survey, bucket_ids) do
    Repo.all(
      from r in CompletedRespondents,
        where: r.survey_id == ^survey.id and r.quota_bucket_id not in ^bucket_ids,
        group_by: [r.quota_bucket_id, r.date],
        order_by: r.date,
        select: {r.quota_bucket_id, r.date, fragment("CAST(? AS UNSIGNED)", sum(r.count))}
    )
  end

  defp add_disposition_percent(
         respondents_count_by_disposition_and_questionnaire,
         total_respondents
       ) do
    respondents_count_by_disposition_and_questionnaire
    |> Enum.map(fn {disposition, counts} ->
      percent_of_disposition =
        case total_respondents do
          0 ->
            0

          _ ->
            respondents_count_by_disposition_and_questionnaire[disposition][:count] /
              (total_respondents / 100)
        end

      counts_with_percent = counts |> Map.put(:percent, percent_of_disposition)
      {disposition, counts_with_percent}
    end)
    |> Enum.into(%{})
  end

  def sanitize_variable_name(s), do: s |> String.trim() |> String.replace(" ", "_")

  def results(conn, %{"project_id" => project_id, "survey_id" => survey_id} = params) do
    project = load_project(conn, project_id)
    survey = load_survey(project, survey_id)

    tz_offset = Survey.timezone_offset(survey)

    questionnaires = (survey |> Repo.preload(:questionnaires)).questionnaires
    has_comparisons = length(survey.comparisons) > 0

    # We first need to get all unique field names in all questionnaires
    all_fields = all_questionnaires_fields(questionnaires, true)

    # The new filters, shared by the index and the downloaded CSV file
    filter = RespondentsFilter.parse(Map.get(params, "q", ""))
    # The old filters are being received by its own specific url params
    # If the same filter is received twice, the old filter is priorized over new one because
    # ?param1=value is more specific than ?q=param1:value
    filter = add_params_to_filter(filter, params)

    filter_where = RespondentsFilter.filter_where(filter, optimized: true)

    respondents =
      Stream.resource(
        fn -> 0 end,
        fn last_seen_id ->
          results =
            from(r1 in Respondent,
              join: r2 in Respondent,
              on: r1.id == r2.id,
              where: r2.survey_id == ^survey.id and r2.id > ^last_seen_id,
              where: ^filter_where,
              order_by: r2.id,
              limit: 1000,
              preload: [:responses, :respondent_group, :questionnaire],
              select: r1
            )
            |> Repo.all()

          case List.last(results) do
            %{id: last_id} -> {results, last_id}
            nil -> {:halt, last_seen_id}
          end
        end,
        fn _ -> [] end
      )

    partial_relevant_enabled = Survey.partial_relevant_enabled?(survey, true)

    render_results(
      conn,
      get_format(conn),
      project,
      survey,
      tz_offset,
      questionnaires,
      has_comparisons,
      all_fields,
      respondents,
      partial_relevant_enabled
    )
  end

  defp all_questionnaires_fields(questionnaires, sanitize \\ false) do
    fields =
      questionnaires
      |> Enum.flat_map(&Questionnaire.variables/1)
      |> Enum.uniq()
      |> Enum.reject(fn s -> String.length(s) == 0 end)

    if sanitize, do: sanitize_fields(fields), else: fields
  end

  defp sanitize_fields(fields),
    do: Enum.map(fields, fn field -> sanitize_variable_name(field) end)

  defp add_params_to_filter(filter, params) do
    filter =
      if params["disposition"],
        do: RespondentsFilter.put_disposition(filter, params["disposition"]),
        else: filter

    filter =
      if params["since"],
        do: RespondentsFilter.put_since(filter, params["since"]),
        else: filter

    filter =
      if params["final"],
        do: RespondentsFilter.put_state(filter, "completed"),
        else: filter

    filter
  end

  defp render_results(
         conn,
         "json",
         _project,
         survey,
         _tz_offset,
         questionnaires,
         has_comparisons,
         _all_fields,
         respondents,
         partial_relevant_enabled
       ) do
    respondents_count = Ask.RespondentStats.respondent_count(survey_id: ^survey.id)

    respondents =
      if has_comparisons do
        respondents
        |> Stream.map(fn respondent ->
          experiment_name =
            if respondent.questionnaire_id && respondent.mode do
              questionnaire =
                questionnaires |> Enum.find(fn q -> q.id == respondent.questionnaire_id end)

              if questionnaire do
                experiment_name(questionnaire, respondent.mode)
              else
                "-"
              end
            else
              "-"
            end

          %{respondent | experiment_name: experiment_name}
        end)
      else
        respondents
      end

    {:ok, conn} =
      Repo.transaction(fn ->
        render(conn, "index.json",
          respondents: respondents,
          respondents_count: respondents_count,
          partial_relevant_enabled: partial_relevant_enabled
        )
      end)

    conn
  end

  defp render_results(
         conn,
         "csv",
         project,
         survey,
         tz_offset,
         questionnaires,
         has_comparisons,
         all_fields,
         respondents,
         partial_relevant_enabled
       ) do
    stats =
      survey.mode
      |> Enum.flat_map(fn modes ->
        modes
        |> Enum.flat_map(fn mode ->
          case mode do
            "sms" -> [:total_sent_sms, :total_received_sms, :sms_attempts]
            "mobileweb" -> [:total_sent_sms, :total_received_sms, :mobileweb_attempts]
            "ivr" -> [:total_call_time, :ivr_attempts]
            _ -> []
          end
        end)
      end)
      |> Enum.uniq()

    # Now traverse each respondent and create a row for it
    csv_rows =
      respondents
      |> Stream.map(fn respondent ->
        row = [respondent.hashed_number]
        responses = respondent.responses

        row = row ++ [Respondent.show_disposition(respondent.disposition)]

        date =
          case responses do
            [] ->
              nil

            _ ->
              responses
              |> Enum.map(fn r -> r.updated_at end)
              |> Enum.max()
              |> Survey.adjust_timezone(survey)
          end

        row =
          if date do
            row ++ [date |> Timex.format!("%b %e, %Y %H:%M #{tz_offset}", :strftime)]
          else
            row ++ ["-"]
          end

        modes =
          (respondent.effective_modes || [])
          |> Enum.map(fn mode -> mode_label([mode]) end)
          |> Enum.join(", ")

        row = row ++ [modes]

        row = row ++ [respondent.user_stopped]

        row =
          row ++
            Enum.map(stats, fn stat ->
              respondent |> respondent_stat(stat)
            end)

        row = row ++ [Respondent.show_section_order(respondent, questionnaires)]

        respondent_group = respondent.respondent_group.name

        row = row ++ [respondent_group]

        questionnaire_id = respondent.questionnaire_id
        mode = respondent.mode

        row =
          if has_comparisons do
            variant =
              if questionnaire_id && mode do
                questionnaire = questionnaires |> Enum.find(fn q -> q.id == questionnaire_id end)

                if questionnaire do
                  experiment_name(questionnaire, mode)
                else
                  "-"
                end
              else
                "-"
              end

            row ++ [variant]
          else
            row
          end

        row =
          if partial_relevant_enabled do
            row ++ [Respondent.partial_relevant_answered_count(respondent, false)]
          else
            row
          end

        # We traverse all fields and see if there's a response for this respondent
        row =
          all_fields
          |> Enum.reduce(row, fn field_name, acc ->
            response =
              responses
              |> Enum.filter(fn response ->
                response.field_name |> sanitize_variable_name == field_name
              end)

            case response do
              [resp] ->
                value = resp.value

                # For the 'language' variable we convert the code to the native name
                value =
                  if resp.field_name == "language" do
                    LanguageNames.for(value) || value
                  else
                    value
                  end

                acc ++ [value]

              _ ->
                acc ++ [""]
            end
          end)

        row
      end)

    append_if = fn list, elems, condition -> if condition, do: list ++ elems, else: list end

    # Add header to csv_rows
    header = ["respondent_id", "disposition", "date", "modes", "user_stopped"]

    header =
      header ++
        Enum.map(stats, fn stat ->
          case stat do
            :total_sent_sms -> "total_sent_sms"
            :total_received_sms -> "total_received_sms"
            :total_call_time -> "total_call_time"
            :sms_attempts -> "sms_attempts"
            :ivr_attempts -> "ivr_attempts"
            :mobileweb_attempts -> "mobileweb_attempts"
          end
        end)

    header = header ++ ["section_order", "sample_file"]
    header = append_if.(header, ["variant"], has_comparisons)
    header = append_if.(header, ["p_relevants"], partial_relevant_enabled)
    header = header ++ all_fields

    rows = Stream.concat([[header], csv_rows])

    filename = csv_filename(survey, "respondents")
    ActivityLog.download(project, conn, survey, "survey_results") |> Repo.insert()
    conn |> csv_stream(rows, filename)
  end

  defp respondent_stat(respondent, :sms_attempts), do: respondent.stats |> Stats.attempts(:sms)
  defp respondent_stat(respondent, :ivr_attempts), do: respondent.stats |> Stats.attempts(:ivr)

  defp respondent_stat(respondent, :mobileweb_attempts),
    do: respondent.stats |> Stats.attempts(:mobileweb)

  defp respondent_stat(respondent, key), do: apply(Stats, key, [respondent.stats])

  def disposition_history(conn, %{"project_id" => project_id, "survey_id" => survey_id}) do
    project = load_project(conn, project_id)
    survey = load_survey(project, survey_id)

    history =
      Stream.resource(
        fn -> 0 end,
        fn last_id ->
          results =
            from(h in RespondentDispositionHistory,
              where: h.survey_id == ^survey.id and h.id > ^last_id,
              order_by: h.id,
              limit: 1000
            )
            |> Repo.all()

          case List.last(results) do
            nil -> {:halt, last_id}
            last_entry -> {results, last_entry.id}
          end
        end,
        fn _ -> [] end
      )

    tz_offset = Survey.timezone_offset(survey)
    offset_seconds = Survey.timezone_offset_in_seconds(survey)

    csv_rows =
      history
      |> Stream.map(fn history ->
        date = Ask.TimeUtil.format(history.inserted_at, offset_seconds, tz_offset)
        [history.respondent_hashed_number, history.disposition, mode_label([history.mode]), date]
      end)

    header = ["Respondent ID", "Disposition", "Mode", "Timestamp"]
    rows = Stream.concat([[header], csv_rows])

    filename = csv_filename(survey, "respondents_disposition_history")
    ActivityLog.download(project, conn, survey, "disposition_history") |> Repo.insert()
    conn |> csv_stream(rows, filename)
  end

  def incentives(conn, %{"project_id" => project_id, "survey_id" => survey_id}) do
    project =
      conn
      |> load_project_for_owner(project_id)

    survey =
      project
      |> assoc(:surveys)
      |> where([s], s.incentives_enabled)
      |> Repo.get!(survey_id)

    csv_rows =
      from(r in Respondent,
        where:
          r.survey_id == ^survey.id and r.disposition == :completed and
            not is_nil(r.questionnaire_id),
        order_by: r.id
      )
      |> Repo.stream()
      |> Repo.stream_preload(:questionnaire)
      |> Stream.map(fn r ->
        [
          r.phone_number,
          experiment_name(r.questionnaire, r.mode),
          csv_datetime(r.completed_at, survey)
        ]
      end)

    header = ["Telephone number", "Questionnaire-Mode", "Completion date"]
    rows = Stream.concat([[header], csv_rows])

    filename = csv_filename(survey, "respondents_incentives")
    ActivityLog.download(project, conn, survey, "incentives") |> Repo.insert()
    {:ok, conn} = Repo.transaction(fn -> conn |> csv_stream(rows, filename) end)
    conn
  end

  def interactions(conn, %{"project_id" => project_id, "survey_id" => survey_id}) do
    project = load_project_for_owner(conn, project_id)
    survey = load_survey(project, survey_id)

    log_entries =
      Stream.resource(
        fn -> {"", 0} end,
        fn {last_hash, last_id} ->
          results =
            from(e in SurveyLogEntry,
              where:
                e.survey_id == ^survey.id and
                  ((e.respondent_hashed_number == ^last_hash and e.id > ^last_id) or
                     e.respondent_hashed_number > ^last_hash),
              order_by: [e.respondent_hashed_number, e.id],
              limit: 1000,
              preload: :channel
            )
            |> Repo.all()

          case List.last(results) do
            nil -> {:halt, {last_hash, last_id}}
            last_entry -> {results, {last_entry.respondent_hashed_number, last_entry.id}}
          end
        end,
        fn _ -> [] end
      )

    csv_rows =
      log_entries
      |> Stream.map(fn e ->
        channel_name =
          if e.channel do
            e.channel.name
          else
            ""
          end

        disposition = disposition_label(e.disposition)
        action_type = action_type_label(e.action_type)
        timestamp = csv_datetime(e.timestamp, survey)

        [
          Integer.to_string(e.id),
          e.respondent_hashed_number,
          interactions_mode_label(e.mode),
          channel_name,
          disposition,
          action_type,
          e.action_data,
          timestamp
        ]
      end)

    header = [
      "ID",
      "Respondent ID",
      "Mode",
      "Channel",
      "Disposition",
      "Action Type",
      "Action Data",
      "Timestamp"
    ]

    rows = Stream.concat([[header], csv_rows])

    filename = csv_filename(survey, "respondents_interactions")
    ActivityLog.download(project, conn, survey, "interactions") |> Repo.insert()
    conn |> csv_stream(rows, filename)
  end

  defp mask_phone_numbers(respondent) do
    %{respondent | phone_number: Respondent.mask_phone_number(respondent.phone_number)}
  end

  defp experiment_name(quiz, mode) do
    "#{questionnaire_name(quiz)} - #{mode_label(mode)}"
  end

  defp questionnaire_name(quiz) do
    quiz.name || "Untitled questionnaire"
  end

  defp mode_label(mode) do
    case mode do
      ["sms"] -> "SMS"
      ["sms", "ivr"] -> "SMS with phone call fallback"
      ["sms", "mobileweb"] -> "SMS with Mobile Web fallback"
      ["ivr"] -> "Phone call"
      ["ivr", "sms"] -> "Phone call with SMS fallback"
      ["ivr", "mobileweb"] -> "Phone call with Mobile Web fallback"
      ["mobileweb"] -> "Mobile Web"
      ["mobileweb", "sms"] -> "Mobile Web with SMS fallback"
      ["mobileweb", "ivr"] -> "Mobile Web with phone call fallback"
      _ -> "Unknown mode"
    end
  end

  defp interactions_mode_label(mode) do
    case mode do
      "mobileweb" -> "Mobile Web"
      _ -> String.upcase(mode)
    end
  end

  defp action_type_label(action) do
    case action do
      nil -> nil
      "contact" -> "Contact attempt"
      _ -> String.capitalize(action)
    end
  end

  defp disposition_label(disposition) do
    case disposition do
      nil -> nil
      _ -> String.capitalize(disposition)
    end
  end

  defp csv_filename(survey, prefix) do
    name = survey.name || "survey_id_#{survey.id}"
    name = Regex.replace(~r/[^a-zA-Z0-9_]/, name, "_")
    prefix = "#{name}-#{prefix}"
    Timex.format!(DateTime.utc_now(), "#{prefix}_%Y-%m-%d-%H-%M-%S.csv", :strftime)
  end

  defp csv_datetime(nil, _), do: ""

  defp csv_datetime(dt, survey) when is_binary(dt) do
    {:ok, datetime, _offset} = DateTime.from_iso8601(dt)
    csv_datetime(datetime, survey)
  end

  defp csv_datetime(dt, %Survey{} = survey) do
    tz_offset = Survey.timezone_offset(survey)
    offset_seconds = Survey.timezone_offset_in_seconds(survey)

    Ask.TimeUtil.format(dt, offset_seconds, tz_offset)
  end

  defp load_survey(project, survey_id) do
    project
    |> assoc(:surveys)
    |> Repo.get!(survey_id)
  end
end
